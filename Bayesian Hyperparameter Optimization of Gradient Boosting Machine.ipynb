{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Hyperparameter Optimization\n",
    "\n",
    "In this notebook we will explore several options for hyperparameter optimization of a machine learning algorithm. We will start with some of the basic methods such as random search, and then proceed to more sophisticated methods using Guassian Processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (5822, 85)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MOSTYPE</th>\n",
       "      <th>MAANTHUI</th>\n",
       "      <th>MGEMOMV</th>\n",
       "      <th>MGEMLEEF</th>\n",
       "      <th>MOSHOOFD</th>\n",
       "      <th>MGODRK</th>\n",
       "      <th>MGODPR</th>\n",
       "      <th>MGODOV</th>\n",
       "      <th>MGODGE</th>\n",
       "      <th>MRELGE</th>\n",
       "      <th>...</th>\n",
       "      <th>ALEVEN</th>\n",
       "      <th>APERSONG</th>\n",
       "      <th>AGEZONG</th>\n",
       "      <th>AWAOREG</th>\n",
       "      <th>ABRAND</th>\n",
       "      <th>AZEILPL</th>\n",
       "      <th>APLEZIER</th>\n",
       "      <th>AFIETS</th>\n",
       "      <th>AINBOED</th>\n",
       "      <th>ABYSTAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  MOSHOOFD  MGODRK  MGODPR  MGODOV  \\\n",
       "0       33         1        3         2         8       0       5       1   \n",
       "1       37         1        2         2         8       1       4       1   \n",
       "2       37         1        2         2         8       0       4       2   \n",
       "3        9         1        3         3         3       2       3       2   \n",
       "4       40         1        4         2        10       1       4       1   \n",
       "\n",
       "   MGODGE  MRELGE    ...     ALEVEN  APERSONG  AGEZONG  AWAOREG  ABRAND  \\\n",
       "0       3       7    ...          0         0        0        0       1   \n",
       "1       4       6    ...          0         0        0        0       1   \n",
       "2       4       3    ...          0         0        0        0       1   \n",
       "3       4       5    ...          0         0        0        0       1   \n",
       "4       4       7    ...          0         0        0        0       1   \n",
       "\n",
       "   AZEILPL  APLEZIER  AFIETS  AINBOED  ABYSTAND  \n",
       "0        0         0       0        0         0  \n",
       "1        0         0       0        0         0  \n",
       "2        0         0       0        0         0  \n",
       "3        0         0       0        0         0  \n",
       "4        0         0       0        0         0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and separate into training and testing sets\n",
    "data = pd.read_csv('data/caravan-insurance-challenge.csv')\n",
    "train = data[data['ORIGIN'] == 'train']\n",
    "test = data[data['ORIGIN'] == 'test']\n",
    "\n",
    "# Extract the labels and format properly\n",
    "train_labels = np.array(train['CARAVAN'].astype(np.int32)).reshape((-1,))\n",
    "test_labels = np.array(test['CARAVAN'].astype(np.int32)).reshape((-1,))\n",
    "\n",
    "# Drop the unneeded columns\n",
    "train = train.drop(columns = ['ORIGIN', 'CARAVAN'])\n",
    "test = test.drop(columns = ['ORIGIN', 'CARAVAN'])\n",
    "\n",
    "# Convert to numpy array for splitting in cross validation\n",
    "features = np.array(train)\n",
    "labels = train_labels\n",
    "\n",
    "print('Train shape: ', train.shape)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search by Hand\n",
    "\n",
    "The first method we can implement is simply random search. Each iteration, choose a random set of model hyperparameters from a search space. Empirically, random search is very effective, returning nearly as good results as grid search with a significant reduction in time spent searching. \n",
    "\n",
    "Random search can be implement in the Scikit-Learn library with the LightGBM Sklearn API. However, this does not support training with early stopping, which is the most effective method for determining the best number of iterations to use. Therefore, we will implement random search ourselves with a defined parameter grid, and using Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "        n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain \n",
    "\n",
    "In random search, as in Bayesian optimization, we have a domain over which we search for the best hyperparameters. In terms of a random or grid search, this is generally known as a hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(30, 151)),\n",
    "    'learning_rate': list(np.logspace(np.log(0.01), np.log(0.2), base = np.exp(1), num = 100)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10))\n",
    "}\n",
    "\n",
    "# Subsampling (only applicable with 'goss')\n",
    "subsample_dist = list(np.linspace(0.5, 1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = 50\n",
    "\n",
    "# Dataframe to hold cv results\n",
    "results = pd.DataFrame(columns = ['params', 'train_scores', 'train', 'valid_scores', 'valid', 'estimators'],\n",
    "                       index = list(range(evals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# Iterate through the specified number of evaluations\n",
    "for i in range(evals):\n",
    "    \n",
    "    k_fold = KFold(n_splits = 5)\n",
    "    \n",
    "    # Randomly sample parameters for gbm\n",
    "    params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n",
    "    \n",
    "    \n",
    "    if params['boosting_type'] == 'goss':\n",
    "        # Cannot subsample with goss\n",
    "        params['subsample'] = 1.0\n",
    "    else:\n",
    "        # Subsample supported for gdbt and dart\n",
    "        params['subsample'] = random.sample(subsample_dist, 1)[0]\n",
    "        \n",
    "        \n",
    "    # Create the model with the parameters\n",
    "    model = lgb.LGBMClassifier(class_weight = params['class_weight'], boosting_type = params['boosting_type'], \n",
    "                               num_leaves = params['num_leaves'], learning_rate = params['learning_rate'], \n",
    "                               subsample_for_bin = params['subsample_for_bin'], min_child_samples = params['min_child_samples'], \n",
    "                               reg_alpha = params['reg_alpha'], reg_lambda = params['reg_lambda'], \n",
    "                               colsample_by_tree = params['colsample_bytree'], subsample = params['subsample'], \n",
    "                               n_estimators = 10000, n_jobs = -1, objective = 'binary', verbose=-1, verbose_eval = False)\n",
    "    \n",
    "    # Empty lists for records\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    number_estimators = []\n",
    "    \n",
    "    # Split the data\n",
    "    for (train_indices, valid_indices) in k_fold.split(features):\n",
    "        \n",
    "        # Training data and validation set\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Fit the model using early stopping\n",
    "        model.fit(train_features, train_labels, eval_set = [(train_features, train_labels), (valid_features, valid_labels)],\n",
    "                  eval_metric = 'auc', eval_names = ['train', 'valid'], early_stopping_rounds = 200, verbose = -1);\n",
    "        \n",
    "        \n",
    "        valid_scores.append(model.best_score_['valid']['auc'])\n",
    "        train_scores.append(model.best_score_['train']['auc'])\n",
    "        number_estimators.append(model.best_iteration_)\n",
    "        \n",
    "    # Average the scores\n",
    "    valid = np.mean(valid_scores)\n",
    "    train = np.mean(train_scores)\n",
    "    estimators = np.mean(number_estimators)\n",
    "    \n",
    "    # Add results to next row in dataframe\n",
    "    results.loc[i, :] = [params, train_scores, train, valid_scores, valid, estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>train_scores</th>\n",
       "      <th>train</th>\n",
       "      <th>valid_scores</th>\n",
       "      <th>valid</th>\n",
       "      <th>estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>[0.8906003308793933, 0.8558370131244537, 0.888...</td>\n",
       "      <td>0.873115</td>\n",
       "      <td>[0.7848923679060665, 0.7501188811188811, 0.761...</td>\n",
       "      <td>0.771327</td>\n",
       "      <td>39.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'class_weight': None, 'boosting_type': 'gbdt'...</td>\n",
       "      <td>[0.8329243889656487, 0.8795007763511014, 0.849...</td>\n",
       "      <td>0.841323</td>\n",
       "      <td>[0.777058056099152, 0.7655244755244756, 0.7653...</td>\n",
       "      <td>0.770526</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>[0.8835527148046349, 0.9256136889845392, 0.919...</td>\n",
       "      <td>0.899154</td>\n",
       "      <td>[0.7862752772341813, 0.7452657342657343, 0.761...</td>\n",
       "      <td>0.770208</td>\n",
       "      <td>59.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'class_weight': None, 'boosting_type': 'gbdt'...</td>\n",
       "      <td>[0.8763761313397329, 0.9047325910738204, 0.890...</td>\n",
       "      <td>0.871686</td>\n",
       "      <td>[0.772133072407045, 0.7643846153846154, 0.7626...</td>\n",
       "      <td>0.769959</td>\n",
       "      <td>63.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>[0.8817968689674888, 0.865761139143768, 0.8735...</td>\n",
       "      <td>0.87193</td>\n",
       "      <td>[0.778382257012394, 0.7474895104895105, 0.7618...</td>\n",
       "      <td>0.769956</td>\n",
       "      <td>90.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  \\\n",
       "22  {'class_weight': 'balanced', 'boosting_type': ...   \n",
       "1   {'class_weight': None, 'boosting_type': 'gbdt'...   \n",
       "17  {'class_weight': 'balanced', 'boosting_type': ...   \n",
       "30  {'class_weight': None, 'boosting_type': 'gbdt'...   \n",
       "32  {'class_weight': 'balanced', 'boosting_type': ...   \n",
       "\n",
       "                                         train_scores     train  \\\n",
       "22  [0.8906003308793933, 0.8558370131244537, 0.888...  0.873115   \n",
       "1   [0.8329243889656487, 0.8795007763511014, 0.849...  0.841323   \n",
       "17  [0.8835527148046349, 0.9256136889845392, 0.919...  0.899154   \n",
       "30  [0.8763761313397329, 0.9047325910738204, 0.890...  0.871686   \n",
       "32  [0.8817968689674888, 0.865761139143768, 0.8735...   0.87193   \n",
       "\n",
       "                                         valid_scores     valid estimators  \n",
       "22  [0.7848923679060665, 0.7501188811188811, 0.761...  0.771327       39.8  \n",
       "1   [0.777058056099152, 0.7655244755244756, 0.7653...  0.770526         41  \n",
       "17  [0.7862752772341813, 0.7452657342657343, 0.761...  0.770208       59.2  \n",
       "30  [0.772133072407045, 0.7643846153846154, 0.7626...  0.769959       63.8  \n",
       "32  [0.778382257012394, 0.7474895104895105, 0.7618...  0.769956       90.8  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = results.sort_values('valid', ascending = False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'dart',\n",
       " 'class_weight': 'balanced',\n",
       " 'colsample_bytree': 0.7333333333333333,\n",
       " 'learning_rate': 0.19403866524532978,\n",
       " 'min_child_samples': 270,\n",
       " 'num_leaves': 74,\n",
       " 'reg_alpha': 1.0,\n",
       " 'reg_lambda': 0.3877551020408163,\n",
       " 'subsample': 0.8080808080808082,\n",
       " 'subsample_for_bin': 180000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Optimization using Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d729f025c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2))}\n",
    "learning_rate_dist = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    learning_rate_dist.append(sample(learning_rate)['learning_rate'])\n",
    "    \n",
    "plt.hist(learning_rate_dist, bins = 10, edgecolor = 'k');\n",
    "plt.title('Learning Rate Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d72e3157f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
    "num_leaves_dist = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    num_leaves_dist.append(sample(num_leaves)['num_leaves'])\n",
    "    \n",
    "plt.hist(num_leaves_dist, bins = 10, edgecolor = 'k');\n",
    "plt.title('Number of Leaves Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Function\n",
    "\n",
    "The objective function will be the cross validation score evaluate over 5 folds. We need to make sure the objective function returns a single, real-value metric. We can return more information in the form of a dictionary where one of the keys must be 'loss' and another must be 'STATUS'. The other keys can hold information such as the hyperparameters used or the evaluation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \n",
    "    global iteration\n",
    "    \n",
    "    iteration += 1\n",
    "    \n",
    "    k_fold = KFold(n_splits = 5)\n",
    "    \n",
    "    # Retrieve the subsample if present otherwise set to 1.0\n",
    "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type\n",
    "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    \n",
    "    model = lgb.LGBMClassifier(n_estimators = 10000, **params, objective = 'binary', n_jobs = -1, verbose = -1)\n",
    "    \n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    number_estimators = []\n",
    "    \n",
    "    start = timer()\n",
    "    for (train_indices, valid_indices) in k_fold.split(features):\n",
    "        \n",
    "        # Training data and validation set\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Fit the model using early stopping\n",
    "        model.fit(train_features, train_labels, eval_set = [(train_features, train_labels), (valid_features, valid_labels)],\n",
    "                  eval_metric = 'auc', eval_names = ['train', 'valid'], early_stopping_rounds = 200, verbose = -1)\n",
    "    \n",
    "        valid_scores.append(model.best_score_['valid']['auc'])\n",
    "        train_scores.append(model.best_score_['train']['auc'])\n",
    "        number_estimators.append(model.best_iteration_)\n",
    "        \n",
    "    end = timer()\n",
    "    \n",
    "    run_time = end - start\n",
    "    \n",
    "    # fmin needs a loss to minimize\n",
    "    valid = -1 * np.mean(valid_scores)\n",
    "    train = -1 * np.mean(train_scores)\n",
    "    \n",
    "    # average number of estimators\n",
    "    estimators = np.mean(number_estimators)\n",
    "\n",
    "    o_f = open(out_file, 'a')\n",
    "    writer = csv.writer(o_f)\n",
    "    \n",
    "    writer.writerow([valid, train, estimators, run_time, params, iteration])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': valid, 'train': train, 'estimators': estimators, \n",
    "            'train_time': run_time, 'status': STATUS_OK, 'params': params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "space = {\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                 {'boosting_type': 'goss'}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_type = {'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n",
    "                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                                                 {'boosting_type': 'goss'}])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': {'boosting_type': 'gbdt', 'subsample': 0.6440506964135775}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(boosting_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': {'boosting_type': 'goss'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(boosting_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.30882886229848516}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.4050590135319103}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample(space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Sampling from the space\n",
    "\n",
    "After finding the boosting type (which is in a nested dictionary), we assign the boosting type to make it a top level value. We use the dictionary get method to find the 'subsample' if it is in the dictionary (indicating the boosting type is 'gbdt' or 'dart') or set it to 1.0 otherwise (if boosting type is 'goss'). The goss boosting type cannot use bagging. \n",
    "\n",
    "This entire step is necessary because of the conditional logic used for the boosting type and subsample ratio.\n",
    "\n",
    "In addition, we can see the other variables in the dictionary. These will change every time we sample the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 0.9027644056192846,\n",
       " 'learning_rate': 0.05087793626126211,\n",
       " 'min_child_samples': 455.0,\n",
       " 'num_leaves': 110.0,\n",
       " 'reg_alpha': 0.20472634108516952,\n",
       " 'reg_lambda': 0.4483519575330225,\n",
       " 'subsample': 0.6622175250143509,\n",
       " 'subsample_for_bin': 100000.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sample(space)\n",
    "subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "x['subsample'] = subsample\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'goss',\n",
       " 'class_weight': 'balanced',\n",
       " 'colsample_bytree': 0.8123618729971422,\n",
       " 'learning_rate': 0.025225385229030017,\n",
       " 'min_child_samples': 45.0,\n",
       " 'num_leaves': 77.0,\n",
       " 'reg_alpha': 0.7615253222246864,\n",
       " 'reg_lambda': 0.8984261853504442,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 140000.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sample(space)\n",
    "subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "x['subsample'] = subsample\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "out_file = 'gbm_trials1.csv'\n",
    "iteration = 1\n",
    "\n",
    "o_f = open(out_file, 'w')\n",
    "writer = csv.writer(o_f)\n",
    "writer.writerow(['loss', 'train', 'estimators', 'train_time', 'params', 'iteration'])\n",
    "o_f.close()\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = 5, trials = trials, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_results = sorted(trials.results, key = lambda x: x['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'estimators': 177.0,\n",
       "  'loss': -0.7697892949519953,\n",
       "  'params': {'boosting_type': 'goss',\n",
       "   'class_weight': 'balanced',\n",
       "   'colsample_bytree': 0.8834589981795999,\n",
       "   'learning_rate': 0.054098718849793385,\n",
       "   'min_child_samples': 470,\n",
       "   'num_leaves': 116,\n",
       "   'reg_alpha': 0.21352364067394936,\n",
       "   'reg_lambda': 0.580483010352201,\n",
       "   'subsample': 1.0,\n",
       "   'subsample_for_bin': 20000},\n",
       "  'status': 'ok',\n",
       "  'train': -0.7926234865603419,\n",
       "  'train_time': 2.1998353635087255},\n",
       " {'estimators': 35.6,\n",
       "  'loss': -0.7684741746940057,\n",
       "  'params': {'boosting_type': 'gbdt',\n",
       "   'class_weight': 'balanced',\n",
       "   'colsample_bytree': 0.8794527985116436,\n",
       "   'learning_rate': 0.12425640932943384,\n",
       "   'min_child_samples': 375,\n",
       "   'num_leaves': 88,\n",
       "   'reg_alpha': 0.8957328991842315,\n",
       "   'reg_lambda': 0.8001853323650469,\n",
       "   'subsample': 0.6663096224369767,\n",
       "   'subsample_for_bin': 300000},\n",
       "  'status': 'ok',\n",
       "  'train': -0.8207163854311504,\n",
       "  'train_time': 2.190121582967917}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('trials.json', 'w') as f:\n",
    "    f.write(json.dumps(trials_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "out_file = 'gbm_trials2.csv'\n",
    "iteration = 1\n",
    "\n",
    "o_f = open(out_file, 'w')\n",
    "writer = csv.writer(o_f)\n",
    "writer.writerow(['loss', 'train', 'estimators', 'train_time', 'params', 'iteration'])\n",
    "o_f.close()\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = 100, trials = trials, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'estimators': 92.4,\n",
       "  'loss': -0.7752686691971785,\n",
       "  'params': {'boosting_type': 'goss',\n",
       "   'class_weight': 'balanced',\n",
       "   'colsample_bytree': 0.8154084655111529,\n",
       "   'learning_rate': 0.050058208748931285,\n",
       "   'min_child_samples': 410,\n",
       "   'num_leaves': 59,\n",
       "   'reg_alpha': 0.3822742768283194,\n",
       "   'reg_lambda': 0.5402917976697761,\n",
       "   'subsample': 1.0,\n",
       "   'subsample_for_bin': 300000},\n",
       "  'status': 'ok',\n",
       "  'train': -0.7935511947345306,\n",
       "  'train_time': 1.6708874441635544},\n",
       " {'estimators': 130.4,\n",
       "  'loss': -0.7751144512486324,\n",
       "  'params': {'boosting_type': 'dart',\n",
       "   'class_weight': 'balanced',\n",
       "   'colsample_bytree': 0.888739417843475,\n",
       "   'learning_rate': 0.06477082605181578,\n",
       "   'min_child_samples': 345,\n",
       "   'num_leaves': 79,\n",
       "   'reg_alpha': 0.4463268333467785,\n",
       "   'reg_lambda': 0.6847851253810945,\n",
       "   'subsample': 0.5705037973587046,\n",
       "   'subsample_for_bin': 80000},\n",
       "  'status': 'ok',\n",
       "  'train': -0.814085792368501,\n",
       "  'train_time': 7.958019970281214}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(trials.results, key = lambda x: x['loss'], reverse = False)\n",
    "results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "out_file = 'gbm_trials3.csv'\n",
    "iteration = 1\n",
    "\n",
    "o_f = open(out_file, 'w')\n",
    "writer = csv.writer(o_f)\n",
    "writer.writerow(['loss', 'train', 'estimators', 'train_time', 'params', 'iteration'])\n",
    "o_f.close()\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = 200, trials = trials, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'estimators': 81.6,\n",
       "  'loss': -0.7749768725306613,\n",
       "  'params': {'boosting_type': 'goss',\n",
       "   'class_weight': 'balanced',\n",
       "   'colsample_bytree': 0.8635474941572823,\n",
       "   'learning_rate': 0.18914378933393633,\n",
       "   'min_child_samples': 420,\n",
       "   'num_leaves': 127,\n",
       "   'reg_alpha': 0.41055129829119585,\n",
       "   'reg_lambda': 0.9641296684543442,\n",
       "   'subsample': 1.0,\n",
       "   'subsample_for_bin': 120000},\n",
       "  'status': 'ok',\n",
       "  'train': -0.8074254510968052,\n",
       "  'train_time': 1.3431077953728163},\n",
       " {'estimators': 279.8,\n",
       "  'loss': -0.774727131513742,\n",
       "  'params': {'boosting_type': 'goss',\n",
       "   'class_weight': 'balanced',\n",
       "   'colsample_bytree': 0.8629181777645893,\n",
       "   'learning_rate': 0.08153348413410726,\n",
       "   'min_child_samples': 455,\n",
       "   'num_leaves': 94,\n",
       "   'reg_alpha': 0.43323582217647516,\n",
       "   'reg_lambda': 0.7889969605386284,\n",
       "   'subsample': 1.0,\n",
       "   'subsample_for_bin': 60000},\n",
       "  'status': 'ok',\n",
       "  'train': -0.8012562358317634,\n",
       "  'train_time': 3.930934024798262}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(trials.results, key = lambda x: x['loss'], reverse = False)\n",
    "results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to use Hyperopt and the Tree Parzen Estimator to optimize the hyperparameters of a gradient boosting machine. Bayesian model-based optimization is more efficient than random search, finding a better set of model hyperparameters in fewer objective function (train-predict-evaluate) calls. In later notebooks we will examine using hyperparameter optimization on additional problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
